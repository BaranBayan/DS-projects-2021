{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as ltb\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159566</td>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159567</td>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159568</td>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159569</td>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159570</td>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic\n",
       "0       Explanation\\nWhy the edits made under my usern...      0\n",
       "1       D'aww! He matches this background colour I'm s...      0\n",
       "2       Hey man, I'm really not trying to edit war. It...      0\n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4       You, sir, are my hero. Any chance you remember...      0\n",
       "...                                                   ...    ...\n",
       "159566  \":::::And for the second time of asking, when ...      0\n",
       "159567  You should be ashamed of yourself \\n\\nThat is ...      0\n",
       "159568  Spitzer \\n\\nUmm, theres no actual article for ...      0\n",
       "159569  And it looks like it was actually you who put ...      0\n",
       "159570  \"\\nAnd ... I really don't think you understand...      0\n",
       "\n",
       "[159571 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      "text     159571 non-null object\n",
      "toxic    159571 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "display(df)\n",
    "df.info()\n",
    "# загрузили данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma = lemmatizer.lemmatize(text)\n",
    "    return \"\".join(lemma).strip()\n",
    "# функция лемматизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    sub = re.sub(r'[^a-zA-Z ]', ' ', text)\n",
    "    lem = sub.split()\n",
    "    return \" \".join(lem)\n",
    "# функция удаления лишних символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clear_text'] = df['text'].apply(lambda x: lemmatize(clear_text(x)))\n",
    "# лемматизировали корпус и убрали лишние слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['clear_text'].values\n",
    "# создали корпус"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "# загрузили стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "# инициализировали TF-IDF-векторайзер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119678,) (39893,)\n",
      "(119678,) (39893,)\n"
     ]
    }
   ],
   "source": [
    "corpus_train, corpus_test = train_test_split(corpus, test_size=0.25, random_state=12345)\n",
    "print(corpus_train.shape, corpus_test.shape)\n",
    "\n",
    "target_train, target_test = train_test_split(df['toxic'], test_size=0.25, random_state=12345)\n",
    "print(target_train.shape, target_test.shape)\n",
    "# разделили корпус на выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = count_tf_idf.fit_transform(corpus_train)\n",
    "features_test = count_tf_idf.transform(corpus_test)\n",
    "# создали матрицу со значениями TF-IDF по корпусу комментариев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 15}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7642714339773453"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "parametrs = {'C': [1, 8, 15]}\n",
    "grid = GridSearchCV(clf, parametrs, cv=5, scoring='f1')\n",
    "grid.fit(features_train, target_train)\n",
    "print(grid.best_params_)\n",
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7820790471034109"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(C=15)\n",
    "model.fit(features_train, target_train)\n",
    "predictions = model.predict(features_test)\n",
    "f1_score(target_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 8, 'max_features': 4}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0022996436731455927"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(random_state = 12345)\n",
    "parametrs = {'max_depth': range (1, 10), \n",
    "            'max_features': range(1, 5)}\n",
    "grid = GridSearchCV(clf, parametrs, cv=5, scoring='f1')\n",
    "grid.fit(features_train, target_train)\n",
    "print(grid.best_params_)\n",
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004873294346978557"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DecisionTreeClassifier(random_state=12345, max_depth=8, max_features=4)\n",
    "model.fit(features_train, target_train)\n",
    "predictions = model.predict(features_test) \n",
    "f1_score(target_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7461362335432169"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ltb.LGBMClassifier(random_state = 12345)\n",
    "model.fit(features_train, target_train)\n",
    "predictions = model.predict(features_test)\n",
    "f1_score(target_test, predictions)\n",
    "#обучили модель без поиска по сетке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 11, 'n_estimators': 10}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state = 12345)\n",
    "parametrs = { 'n_estimators': range (10, 51, 10),\n",
    "              'max_depth': range (1,13, 2) }\n",
    "grid = GridSearchCV(clf, parametrs, cv=5, scoring='f1')\n",
    "grid.fit(features_train, target_train)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier(random_state=12345, n_estimators=10, max_depth=11) \n",
    "model.fit(features_train, target_train) \n",
    "predictions = model.predict(features_test)\n",
    "f1_score(target_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучили разные модели и получили величины метрики f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате модель для классификации комментариев подобрана - логистическая регрессия с параметром С=15. F1-мера на тестовой выборке = 0,78. Нужный результат достигнут. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
